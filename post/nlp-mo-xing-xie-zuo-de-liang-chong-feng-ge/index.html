<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title># NLP 模型写作的两种风格 | Note~</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://iclementine.github.io/favicon.ico?v=1596674689171">
<link rel="stylesheet" href="https://iclementine.github.io/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="NLP 模型写作的两种风格， 算是对个人代码风格和抽象方式的一种探索和总结。


今天我们来说说 NLP 模型写作的两种风格。首先我们来说说一个 NLP 模型就其输入输出而言， 究竟在干什么？ 从输入输出的转换来说有几种可能。

序列标注（..." />
    <meta name="keywords" content="" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://iclementine.github.io">
        <img src="https://iclementine.github.io/images/avatar.png?v=1596674689171" class="site-logo">
        <h1 class="site-title">Note~</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      写笔记的地方啦
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://iclementine.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title"># NLP 模型写作的两种风格</h2>
            <div class="post-date">2020-08-05</div>
            
            <div class="post-content" v-pre>
              <p>NLP 模型写作的两种风格， 算是对个人代码风格和抽象方式的一种探索和总结。</p>
<!-- more -->
<figure data-type="image" tabindex="1"><img src="https://iclementine.github.io/post-images/1596646110538.png" alt="" loading="lazy"></figure>
<p>今天我们来说说 NLP 模型写作的两种风格。首先我们来说说一个 NLP 模型就其输入输出而言， 究竟在干什么？ 从输入输出的转换来说有几种可能。</p>
<ol>
<li>序列标注（输入一个词序列，转写出等长标签序列）</li>
<li>结构预测（输入一个词序列，输出树结构， 也可能通过输出转移动作序列再来重构）</li>
<li>机器翻译（输入一个词序列， 输出一个词序列）</li>
<li>语言模型(可以视为机器翻译的 identical 版本)</li>
</ol>
<p>从输入输出的角度来说，都是一个词汇下的序列， 或者是 tagset 下的序列， 或者是 word_set 下的序列。因此最为直接的形式就是使用这些词的原始表示， 比较间接一点的是把词汇映射到一个整数集合， 一般是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>3</mn><mo separator="true">,</mo><mi mathvariant="normal">∥</mi><mi>V</mi><mo>−</mo><mn>1</mn><mi mathvariant="normal">∥</mi></mrow><annotation encoding="application/x-tex">{0,1,2,3, \|V-1\|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∥</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span><span class="mord">∥</span></span></span></span></span> 这样的整数序列构成的集合， 好方便做计算。因此一般会有以下的数据处理的流程：</p>
<pre><code class="language-text">input_words -&gt;
input_ids -&gt;
embs -&gt; (transformation)* -&gt;
probs -&gt;
(sampler) -&gt;
	output_ids -&gt;
	output_words
(criteron) -&gt;
	loss
</code></pre>
<p>看起来我们可以比较方便地把其中 <code>input_words -&gt; input_ids</code> 和 <code>output_ids -&gt; output_words</code> 两步单独抽厘出来， 作为词典的功能来实现， 并且把中间的部分作为模型的核心计算单元而存在。这种风格可以被称为 Operator 风格。</p>
<h2 id="operator-风格">Operator 风格</h2>
<p>将这种风格称为 Operator 风格的原因在于这种风格将核心的计算单元抽厘出来， 使得这个模型的功能很纯粹， 它可以合适地被置入计算图中， 它的 Constructor 参数只包含了必要的整数， 来指定一些维度模型结构信息， 以及一些更加基础的 Operator。它的行为尽可能地模仿一个成熟的， 模块化了的模块的行为， 亦即， 它甚至能够作为更大的 Operator 的一部分整合进去， 因此其内部的过程被封装， 接受一定的输入就返回一定的输出， 应该尽可能地不产生副作用。</p>
<p>（P.S. 我们一般认为的对参数只读的函数也就是如此， 不过因为在计算图的话语下来叙述， 就使用 Operator 的说法， 如果一个对象接受图中节点的输出张量（当然， 在 dynet 话语下， 我们会说 Expression） 作为输入， 又可以返回张量 (Expression), 那么它就是一个比较合格的 Operator）.</p>
<p>比如说一个例子：</p>
<pre><code class="language-python">class OneLayerMLP(object):
  def __init__(self, model, num_input, num_hidden, num_out, act=dy.tanh):
    pc =  model.add_subcollection(&quot;one_layer_mlp&quot;)
    self.W1 = pc.add_parameters((num_hidden, num_input))
    self.W2 = pc.add_parameters((num_out, num_hidden))
    self.b1 = pc.add_parameters((num_hidden))
    self.b2 = pc.add_parameters((num_out))
    self.pc = pc
    self.act = act
    self.spec = (num_input, num_hidden, num_out, act)

  def __call__(self, input_exp):
    W1 = dy.parameter(self.W1)
    W2 = dy.parameter(self.W2)
    b1 = dy.parameter(self.b1)
    b2 = dy.parameter(self.b2)
    g = self.act
    return dy.softmax(W2*g(W1*input_exp + b1)+b2)

  # support saving:
  def param_collection(self): return self.pc

  @staticmethod
  def from_spec(spec, model):
    num_input, num_hidden, num_out, act = spec
    return OneLayerMLP(model, num_input, num_hidden, num_out, act)
</code></pre>
<p>这种 Operator 风格不仅仅模仿成熟模块的优美， 容易组合出更大模型的特点， 还会存在一个性能上的好处： 如果 <code>input_words -&gt; input_ids</code> 这样的过程也被包含在模型的调用过程中， 那么势必会消耗一些时间， 比较追求性能的用户会觉得这些都算是特征提取的一部分， 而且在训练过程中， 一份数据会被迭代多次， 比如说 (5000)， 如果特征一开始就被提取好了， 那么这将会节省出很多的时间。</p>
<p>比如说从 <code>input_words 到 input_ids</code> 需要字典查找， 而且可能 fallback 到 <code>&lt;unk&gt;</code> 上，虽然每一个词的查找都只需要 <code>O(1)</code> 的时间， 不过有可能也值得争取啦。而 <code>output_ids -&gt; output_words</code> 则只有在 test 时候才会使用。训练时候标签也是输入。</p>
<p>但是性能提升的同时也是有代价的， 代价就是模型更加封装了，我们不能方便地以人类可知觉的方式检视模型内部的状态， 因为模型不能方便地访问词汇表， 因为模型的构造函数只得到了必要的维度信息， 所以它们不知道输入和输出的词汇表， 得到了输出之后需要另外找到原先的输出词典来查找， 而且如果模型是 batched 的， 那么还需要经过两种索引才能得到一个 scalar 用于反查， 也是比较麻烦的。而且这样做往往需要我们在核心计算单元之外另外再封装一层， 用来装模型的词汇表， 以及为核心计算单元预先做好整个数据集的预处理， 一路到 batch 的构建。然后把这些 batch 输入核心计算单元。并且把从核心计算单元的输出反转为输出词汇的序列，一般还需要 batch 和不 batch 两种版本， 一个用于跑分， 一个用于 Demo。</p>
<p>但是 <em>核心计算单元</em>, 它就真的可以完全脱离词汇表吗？ 经过仔细思考， 我们会发现， 其实并不能， 而且强行这样做还会带来写模型的许多不便， 虽然也能写啦， 就是写起来比较别扭。比如说我们举几个例子：</p>
<ol>
<li>
<p>word-embedding dropout 的事情上， 有一种策略是为了使得 <code>&lt;unk&gt;</code> 的向量也能得到充分的训练， 并且不那么生硬地在 vocab 上做 max_size 或者 min_freq 的切断， 而是使用随机化手段， 使得低频词更有可能被随机替换为 <code>&lt;unk&gt;</code>, 这并不是 neural network 意义上的 drop-out, 而是可以视为以往的频率估计中 smoothing 的某种传承。 那么就需要访问一个词的 freq 信息， 而且这需要在模型内部动态地处理， 所以还是有一个 vocab， 或者 freq_dist 对象在模型内部比较方便。</p>
</li>
<li>
<p>另外， 程序的执行需要有一些流控制是和词汇的意义有关的， 比如说文本生成模型在生成了 <code>&lt;/s&gt;</code> 之后就停止， 如果不是把词典传给模型使得其能够决策何时停止， 那么就需要把这个特殊的终止符号对应的整数传给模型， 而决定这个终止符对应的整数还是要用到词汇， 强行要求模型简洁， 不包含词汇， 反而会增加耦合， 它要求模型中的 <code>&lt;eos&gt;</code> 和词典中的 <code>&lt;eos&gt;</code> 对应的整数相同， 这增加了麻烦的程度。</p>
</li>
<li>
<p>模型持有词汇表使得检视模型的状态和输出更加容易。比如说语言生成模型， 直接生成一个句子比输出一个 <code>(seq_len, batch_size)</code> 的矩阵更能让人对模型的表现有知觉的评价， 比如模型是否正常， 内部是否犯了什么错误。比如 parser 模型， 直接打印出句子的文本形式和 <code>head -&gt; dependent: deprel</code> 比输出一系列的数字转移动作， 或者转移动作更能让然直观地看出出现了什么错误。当然， 我已经决定了比较好的形式是 <code>form, upos, xpos, deprel, transitions</code> 的形式来表现句子， 这样对于模型和人都比较地方便。</p>
</li>
</ol>
<h2 id="battery-included-风格">Battery-included 风格</h2>
<p>通过以上的一些讨论， 我来表示以下个人的一些爱好。那就是把模型写得如同一个模型， 它本身就是为了完成一项具体的任务或者工作而设计的， 因此节制地而又称职地完成其任务就是最好的选择。而不必考虑这个模型本身能过多大程度地被复用， 因为很多模型都是端到端的， 真的能够复用是少之又少的。你只需要熟悉这个模型的思想， 换什么库， 用什么形式的数据都能具体地实现一番就可以了。</p>
<p>而对于一些很常见的操作， 它们不是一项具体的任务， 把一个操作所需要的实体封装一下， 成为一个 Operator 是很常见也是很简单的操作， 这和我们把模型写得完整， battery included 是不矛盾的。比如说在我们的模型中常常用到的功能模块， 我们也会单独封装成一个模块。比如 OneLayerMLP, 比如一些官方封装的 LSTMBuilder, BiRNNBuilder, 而且这些封装很直观而又不失灵活性， 这是值得我们学习的， 以及我自己也封装了一个 BiLinear 用于图解析算法。这些都是很自然的事情。</p>
<p>这样我们就不需要一个两层封装的模型， 而是一个单层的模型。模型会自己处理 word2idx, idx2word 之类的事情。这么说来， 突然我发现自己从对 <code>torchtext</code> 的崇拜中毕业了呢。Field 基本上可以不用了， 对于具体的数据自己写 parser 就可以了， 而 <code>Vocab</code> 类则是我从 <code>torchtext</code> 中学到的很宝贵的东西， 如何去优雅高效地封装一个词典， 因为模型也要用到它， 所以好好实现一个词典类很重要。</p>
<p>另外， <code>Vocab</code> 类持有词向量也是一个比较好的设计， 这样在建立模型中的 <code>lookup parameters</code> 的时候就可以逐个把这些向量复制过去了。具体的操作是 <code>word -&gt; vector</code>, 然后通过 <code>word -&gt; id -&gt; lookup[id] -&gt; vector</code> 的路径复制过去。</p>
<p>Warning: 封装也会失去灵活性哦。比如说当我想要两个单隐藏层的 MLP 共享 hidden layer, 那么干脆就不要了， 直接用两个 Linear 或者直接写 <code>Wx+b</code> 好了。</p>

            </div>
            
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>






  </body>
</html>
